% created on 2019-12-13
% @author : bmazoyer

%% Lines to compile only this capter
\documentclass[11pt, twoside, a4paper, openright]{report}
\input{../source/packages.tex}

\begin{document}
%%

\graphicspath{ {../figures/mocks_ana/} }

\chapter{Validation des mocks}
\minitoc
\newpage
\thispagestyle{fancy}


\textbf{Ce chapitre a pour vocation de présenter l'analyse menée sur les mocks, afin de valider leur construction et le choix des différents paramètres.
Nous présentons d'abord les estimateurs des diverses fonctions de corrélations, puis les modèles ajustés sur celles-ci.
Nous donnons premièrement le modèle ajusté sur les données et présenté dans \textcite{prov}, puis le modèle modifié ajusté sur les mocks.
Enfin, nous présentons l'analyse des 100 réalisations de mocks produites. L'analyse des données est présentée dans le chapitre suivant.
Ces analyses sont produites avec le code \texttt{picca}. Ce code permet de calculer les champs $\delta$, les fonctions de corrélations, les matrices de distorsion, les matrices de covariance ainsi que de réaliser l'ajustement du modèle.}

\section{Les estimateurs}
Nous présentons ici les estimateurs utilisés pour calculer la fonction d'auto-corrélation du \lya{}, la fonction de corrélation croisée \lya{}-QSO, ainsi que la fonction d'auto-corrélation d'objets ponctuels tels les quasars.
% Dans le cas des objets ponctuels, la seule information nécessaire au calcul de la fonction de corrélation est le catalogue fournissant pour chaque objet l'ascension droite, la déclinaison et le redshift.
\textbf{Dans le cas des objets ponctuels, il est nécessaire de produire un catalogue aléatoire d'objets qui prend en compte la complétude du relevé, ainsi que les effets liés à la sélection des cibles afin de calculer correctement la fonction d'auto-corrélation. % Un tel catalogue est produit pour chaque réalisation des mocks.
}
% Pour le calcul des fonctions de corrélation impliquant le \lya{}, il est nécessaire de calculer au préalable le champ $\delta_F$ (équation~\ref{eq:deltaF}).
\textbf{Pour le calcul des fonctions de corrélation impliquant le \lya{}, il n'est pas utile d'avoir un catalogue aléatoire. En effet, le champ $\delta_F$ nous donne directement accès, à un biais près, au contraste de densité. Nous pouvons donc calculer la fonction de corrélation avec une formule du type de l'équation~\ref{eq:def_cf}, ce qui n'est pas possible dans le cas des objets ponctuels.}
Pour le calcul du champ $\delta_F$, nous distingons deux cas : le cas où nous analysons les \emph{raw mocks} et le cas où nous analysons les \emph{cooked mock} ou les données.
Les raw mocks (les mocks bruts, non préparés) désignent les mocks avant d'avoir tourné le code \texttt{quickquasars}.
Les fonctions de corrélation sont alors calculées en utilisant directement la fraction de flux transmise $F$ donnée dans les fichiers de transmissions. Etant donné que pour chaque forêt, nous avons directement accès à $F$, le champ $\delta_F$ est donné par
\begin{equation}
  \delta_F = \frac{F}{\overline F(z)} - 1 \; .
\end{equation}
Les cooked mocks quant à eux désignent les mocks après avoir appliqué \texttt{quickquasars} à chaque forêt. Dans ce cas, comme pour les données, la fraction de flux transmise $F$ n'est pas accessible. Il faut alors utiliser la procédure d'ajustement du continuum décrite dans la section~\ref{subsec:calcul_delta}.
Une fois le champ $\delta_F$ calculé, nous pouvons calculer les différentes fonctions de corrélation.

\subsection{L'auto-corrélation \lya{}$\times$\lya{}}
Comme expliqué dans le chapitre d'introduction, la fonction d'auto-corrélation du champ d'absorption $F$ du \lya{} est définie comme
\begin{equation}
  \xi_{\mathrm{Ly}\alpha}(\vec r) = \langle \delta_F(\vec{r'}) \delta_F(\vec{r} + \vec{r'}) \rangle \; .
\end{equation}
% Elle peut-être mise sous la forme
Nous pouvons utiliser comme estimateur
\begin{equation}
 \hat \xi(\vec r) = \langle \delta_i \delta_j \rangle \; ,
\end{equation}
où $\langle .\rangle$ désigne la moyenne sur tous les pixels $i$ et $j$ qui vérifient $\vec r_{ij} = \vec r$.
Afin d'estimer $\xi_{\mathrm{Ly}\alpha}$, une grille en $(\rpar{}, \rperp{})$ est créée.
% Chaque bin de cette grille fait $(\SI{4}{\perh\Mpc})^3$.
Les bins mesurent \SI{4}{\perh\Mpc} dans chaque direction.
% Cette taille de bin permet à la fois de regrouper suffisamment de pixels pour avoir une bonne statistique dans chaque bin, mais aussi d'avoir suffisamment de bins pour résoudre correctement la région du pic BAO (\#prov est-ce que c'est vrai ? toujours la meme quantite d'info, donc ca ameliore pas le fit?).
\textbf{Cette taille de bin est choisie de façon à avoir suffisamment de bins pour résoudre correctement la région du pic BAO, mais aussi de façon à ne pas avoir trop de bins pour le calcul de la matrice de covariance. La taille de bin choisie et la gamme en $r$ utilisée produisent déjà une matrice de covariance de \num{2500}$\times$\num{2500} bins. }
% Une fois cette grille construite, nous transformons les coordonnées $(\alpha, \delta, z)$ des pixels $\delta_F$ en distances.
Une fois cette grille construite, la séparation de chaque paires $(\Delta \theta, \Delta z)$ est transformée en distance comobile afin d'obtenir la séparation $(\rpar{}, \rperp{})$ :
\begin{equation}
  \left\{
    \begin{array}{l}
      \rpar{} = \left[D_C(z_i) - D_C(z_j)\right] \cos\left(\frac{\Delta \theta}{2}\right) \; , \\
      \rperp{} = \left[D_M(z_i) - D_M(z_j)\right] \sin\left(\frac{\Delta \theta}{2}\right) \; ,
    \end{array}
  \right. 
\end{equation}
où $D_C$ est la distance comobile le long de la ligne de visée, et $D_M$ la distance comobile transverse (voir section~\ref{subsec:descri_mod}, paragraphe \emph{Les distances}). $z_i$ et $z_j$ sont les redshifts des pixels $i$ et $j$.
Puis, pour chaque bin $A$ de la grille en $(\rpar{}, \rperp{})$, toutes les paires de pixels dont la distance de séparation se trouve dans ce bin $A$ sont considérées. La fonction de corrélation dans ce bin est alors donnée par
\begin{equation}
  \label{eq:xiff}
  \xi_A = \frac{
    \sum\limits_{(i,j)\in A} w_i w_j \delta_i \delta_j
  }{
    \sum\limits_{(i,j)\in A} w_i w_j
  }
  \; ,
\end{equation}
% où $w_i$ est le poids associé au pixel $i$. Les poids utilisés dans le calcul des fonctions de corrélation sont les poids définis dans l'équation~\ref{eq:weights}, corrigés par la dépendance en redshift du champ d'absorption du \lya{}. Le champ du \lya{} varie comme
% \begin{equation}
%   \delta_{\mathrm{Ly}\alpha}(z) = \delta_{\mathrm{Ly}\alpha}(0) \frac{b_{\mathrm{Ly}\alpha}(z) G(z)}{b_{\mathrm{Ly}\alpha}(0)G(0)} \; .
% \end{equation}
% Le facteur de croissance varie comme $G(z) \propto (1+z)^{-1}$, et le biais du \lya{} comme $b_{\mathrm{Ly}\alpha}(z) \propto (1+z)^{\gamma}$.
% Le champ $\delta_{\mathrm{Ly}\alpha}(z)$ est donc proportionnel à $(1+z)^{\gamma - 1}$. Le paramètre $\gamma = 2.9$ est mesuré sur le spectre de puissance à une dimension du \lya \autocite{McDonald2004} (\#prov j'ai pas trouvé dans le papier à quel endroit était donnée la mesure).
% Ainsi, les poids utilisés dans l'équation~\ref{eq:xiff} sont donnés par
% \begin{equation}
%   \label{eq:weights2}
%   w_i = (1+z)^{\gamma - 1} \hat w_i \; ,
% \end{equation}
% où $\hat w_i$ sont les poids définis dans l'équation~\ref{eq:weights}.
% Le calcul de la fonction de corrélation est donc effectué à l'aide d'une double boucle sur les pixels.
où $w_i$ est le poids associé au pixel $i$. Ces poids sont définis dans l'équation~\ref{eq:weights2}. La correction de la dépendance en redshift du biais du \lya{} permet de donner plus de poids aux pixels à grand redshift, où l'amplitude de la fonction de corrélation est plus importante.
Le calcul de la fonction de corrélation est donc effectué à l'aide d'une double boucle sur les pixels.
Afin de réduire le temps de calcul, la fonction de corrélation est calculée uniquement pour les paires de pixels pour lesquelles $\rpar{}$ et $\rperp{}$ sont inclus dans $[0 \, ; 200] \si{\perh\Mpc}$. La fonction de corrélation est donc calculée dans $50 \times 50 = \num{2500}$ bins.
Les paires formées par des pixels provenant de la même forêt sont exclues du calcul afin d'éviter que les erreurs sur l'ajustement du continuum biaisent la mesure de la fonction de corrélation.

L'analyse \lya{} des données complète d'eBOSS \autocite{prov} utilise deux fonctions de corrélation distinctes : les fonctions de corrélation \lyalya{}$\times$\lyalya{} et \lyalya{}$\times$\lyalyb{}.
La région \lyb{} ne fournit pas suffisamment de pixels pour pouvoir calculer la fonction de corrélation \lyalyb{}$\times$\lyalyb{} et y détecter le pic BAO. De plus, le gain de statistique que représente cette fonction de corrélation est négligeable.
L'utilisation de ces deux fonctions de corrélation permet de mesurer la position du pic BAO avec une plus grande précision. Dans ce manuscrit, nous nous intéressons à la mesure de $b_{\mathrm{Ly}\alpha}$ et $\beta_{\mathrm{Ly}\alpha}$ afin de construire correctement nos mocks. Pour limiter les potentielles systématiques, nous considérons donc uniquement la fonction de corrélation \lyalya{}$\times$\lyalya{}.
Le graphique de droite de la figure~\ref{fig:pixel_number} présente les distributions pondérées en redshift des paires \lyalya{}$\times$\lyalya{} et \lyalya{}$\times$\lyalyb{}. L'analyse des données DR16 présentée dans ce manuscrit considère donc uniquement la distribution indiquée en orange.


\subsection{La corrélation croisée \lya{}$\times$QSO}
Nous donnons dans cette section l'estimateur de la fonction de corrélation croisée \lya{}$\times$QSO. De la même manière que précédemment, le calcul s'effectue dans des bins en $(\rpar{}, \rperp{})$. L'estimateur de la fonction de corrélation dans le bin $A$ est donnée par
\begin{equation}
  \label{eq:xiqf}
\hat  \xi_A = \frac{
    \sum\limits_{(i,j)\in A} w_i w_j \delta_i
  }{
    \sum\limits_{(i,j)\in A} w_i w_j
  }
  \; ,
\end{equation}
où l'indice $i$ court sur les pixels des forêts et $j$ sur les quasars pour lesquels la distance de séparation est comprise dans le bin $A$. Les paires $ij$ où le pixel $i$ appartient à la forêt du quasar $j$ sont rejetées.
% Aussi, comme pour le \lya{}, les poids $w_j$ associés aux quasars incluent la dépendance avec le redshift du biais des quasars. Similairement à l'équation~\ref{eq:weights2}, ils sont définis comme
Similairement au \lya{}, les poids $w_j$ associés aux quasars favorisent les quasars à plus grand redshift. Ces poids sont définis comme
\begin{equation}
  \label{eq:weights3}
  w_j = \left(\frac{1 + z_j}{1 + \num{2.25}}\right)^{\gamma_{\textsc{QSO}}} \; ,
\end{equation}
où $\gamma_{\textsc{QSO}} = \num{1.44} \pm \num{0.08}$ \autocite{Bourboux2019}. Dans le calcul de la fonction de corrélation croisée \lya{}$\times$QSO, nous nous restreignons aux paires pour lesquelles $\rperp{} \in [0 \, ; 200] \si{\perh\Mpc}$.
Contrairement à l'auto-corrélation du \lya{}, la fonction de corrélation croisée n'est pas symétrique en $\rpar{}$.
\textbf{Cette assymétrie est produite par l'erreur systématique sur la mesure du redshift des quasars, et aussi par le rayonnement des quasars qui n'est pas isotrope.
La fonction de corrélation croisée est donc calculée dans les bins pour lesquels $\rpar{} \in [-200 \, ; 200] \si{\perh\Mpc}$. Ceci représente $100 \times 50 = \num{5000}$ bins.
}


\subsection{Le spectre de puissance à une dimension}
Comme expliqué dans le chapitre précédent, nous ajustons le spectre de puissance $P_{s}$ appliqué à $\delta_s$ de façon à obtenir un spectre de puissance à une dimension $P^{\mathrm{1D}}$ en accord avec les données. Nous présentons donc maintenant la mesure de ce spectre de puissance sur les raw mocks. Pour chaque forêt, nous appliquons une transformation de Fourier au champ $\delta_F$ afin d'obtenir le champ $\delta_k$. Puis, le spectre de puissance de cette forêt est obtenu comme
\begin{equation}
\hat  P^{\mathrm{1D}}(k) = \langle \delta_k^2 \rangle \; .
\end{equation}
Nous répétons cette procédure pour toutes les forêts, puis le spectre de puissance total est obtenu comme la moyenne du spectre de puissance de chaque forêt.

En ce qui concerne les données et les cooked mocks, la mesure du spectre de puissance à une dimension est plus complexe. Un certain nombre d'effets liés à la mesure doivent être pris en compte. En particulier, le bruit et la résolution doivent être estimés et pris en compte pour ne pas fausser l'estimation du spectre de puissance. Cette analyse est détaillée dans~\textcite{Chabanier2018}.

\subsection{La fonction de corrélation à une dimension}
\textbf{
  Nous présentons maintenant l'estimateur de la fonction de corrélation à une dimension. Similairement au $P^{\mathrm{1D}}$, cette fonction de corrélation est calculée en considérant uniquement les paires de pixels appartenant à la même forêt.
  L'estimateur de la fonction de corrélation est donné par
  \begin{equation}
\hat    \xi^{\mathrm{1D}}_A  = \frac{
      \sum\limits_{(i,j)\in A} w_i w_j \delta_i \delta_j
    }{
      \sum\limits_{(i,j)\in A} w_i w_j
    }
    \; ,
  \end{equation}
  où $w_i$ est le poids associé au pixel $i$. Ces poids sont définis dans l'équation~\ref{eq:weights2}
  La fonction de corrélation à une dimension $\xi^{\mathrm{1D}}$ permet de mettre en avant les autres absorbeurs présents dans le champ d'absorption utilisé pour calculer le champ $\delta_F$. Ces absorbeurs produisent des pics dans la fonction de corrélation à une dimension. Ces pics sont aussi visibles dans les bins le long de la ligne de visée de la fonction de corrélation à trois dimensions (voir l'explication de la matrice des métaux, section~\ref{subsec:model_donnees}). 
  Elle est très affectée par les distorsions dues à l'ajustement du continu \#prov
}


\subsection{L'auto-corrélation QSO$\times$QSO}
To write



\section{Les matrices de distorsion}
\label{sec:calcul_dmat}
L'ajustement du continuum nécessaire au calcul du champ $\delta_F$ dans les données et les coocked mocks biaise le champ mesuré. Cependant, grâce à la transformation (équation~\ref{eq:deltaF3}) décrite dans la section~\ref{subsec:projdelta}), l'effet sur la fonction de corrélation peut-être pris en compte.
% L'idée est la suivante : plutôt que d'essayer de corriger la distorsion induite par l'ajustement du continuum sur la fonction de corrélation, cette distorsion est calculée et appliquée au modèle qui est ajusté aux données (\#prov pourquoi on inverse pas la dmat?).
L'idée est la suivante : modéliser la distorsion induite par l'ajustement du continuum et par la transformation~\ref{eq:deltaF3} sur la fonction de corrélation, appliquer cette distorsion au modèle, puis ajuster le modèle ``distordu'' aux données.
% L'ajustement du continuum et la transformation~\ref{eq:deltaF3} induisent des corrélations le long de la ligne de visée. 
\textbf{L'effet le plus important de l'ajustement du continuum et de la transformation~\ref{eq:deltaF3} est de forcer la moyenne et la pente de chaque région spectrale à être nulle. Ceci induit des corrélations entre les pixels d'une même région spectrale, et donc le long de la ligne de visée.}
Ainsi, au premier ordre, nous pouvons considérer que chaque $\delta_F$ après distorsion d'une forêt est une combinaison linéaire de tous les $\delta_F$ avant distorsion de cette forêt.
La fonction de corrélation distordue dans le bin $A$ peut alors être reliée à la vraie fonction de corrélation comme
\begin{equation}
  \xi_{\mathrm{distorsion}}(A) = \sum_{B} D_{AB}\xi_{vraie}(B) \; , 
\end{equation}
où $D_{AB}$ est appelée la \emph{matrice de distorsion}. Celle-ci s'exprime en fonction du terme $\eta_{ij}^q$, défini dans l'équation~\ref{eq:proj1}. Pour l'auto-corrélation, elle s'exprime comme
\begin{equation}
  \label{eq:dmat}
  D_{AB} = \frac{1}{W_A} \sum_{(i,j)\in A} w_i w_j \left( \sum_{(i',j')\in B} \eta_{ii'} \eta_{jj'} \right) \,
\end{equation}
où $W_{A} = \sum_{(i,j)\in A} w_i w_j$ est le poids du bin A. Pour la corrélation croisée, la matrice de distorsion est donnée par
\begin{equation}
  \label{eq:xdmat}
  D_{AB} = \frac{1}{W_A} \sum_{(i,j)\in A} w_i w_j \left( \sum_{(i',j')\in B} \eta_{ii'} \right) \, .
\end{equation}
Comme précédemment, les indices $i$ correspondent aux pixels des forêts, et $j$ aux quasars. A cause de la double somme, le calcul de la matrice de distorsion est très long. Afin de rendre possible l'estimation de cette dernière, le calcul est fait sur \SI{1}{\percent} des paires, tirées au hasard (\#prov Bautista 2017 montre que c'est OK avec 5\%, est-ce que y a une etude qui montre que c'est ok avec 1\% ? Dans DR14, ils utilisent 5\% je pense).
% L'étude présentée dans~\textcite{bautista_measurement_2017}

La figure~\ref{prov} (\#prov faire la figure) montre l'effet de la distorsion sur l'auto-corrélation \lya{}$\times$\lya{} dans les mocks. La fonction de corrélation est présentée dans quatres gammes en $\mu$. La courbe noire indique la fonction de corrélation calculée sur les raw mocks, et la courbe bleu la fonction de ....
\#prov Refaire la figure 11 de Bautista : CF des raw mocks (stack) + CF sur les coocked mocks (stack) avec le fit sur les raw + fit * DM


La matrice de distorsion est un objet uniquement géométrique. Son calcul est indépendant du champ $\delta_F$. Elle ne dépend uniquement de la géométrie du relevé et de la distribution des poids.
La figure~\ref{prov} montre la différence entre ... \#prov montrer la difference du fit d'une réa avec sa DM et avec une autre DM et/ou le stack de 10 avec les 10 DM ou dix fois la meme. Est-ce que les DM eboss-0.0 et eboss-0.2 sont différentes ?\\
Ainsi, il n'est pas nécessaire de calculer la matrice de distorsion pour les 100 réalisations des mocks.
Dans l'analyse présentée dans la suite de ce chapitre, nous calculons la matrice de distorsion pour l'auto-corrélation et pour la corrélation croisée une seule fois (\#prov ou une fois pour chaque run de quickquasars?). L'ajustement de chaque fonction de corrélation utilise une de ces deux matrices de distorsion.


\section{Les matrices de covariance}
Afin de réaliser l'ajustement de chaque fonction de corrélation, nous avons besoin de calculer les matrices de covariance associées à ces fonctions de corrélation. La covariance de la fonction de corrélation $\xi$ dans le bin $A$ et de $\xi$ dans le bin $B$ est définie comme
\begin{equation}
  C_{AB} = \langle \xi_A \xi_B \rangle - \langle \xi_A \rangle \langle \xi_B \rangle \; .
\end{equation}
De cette matrice de covariance, la matrice de corrélation est définie comme
\begin{equation}
  Corr_{AB} = \frac{C_{AB}}{\sqrt{C_{AA} C_{BB}}} \; .
\end{equation}
La matrice de corrélation donne la corrélation, comprise dans $[-1 \, ; 1]$, d'un bin A avec un bin B.
Afin d'estimer la matrice de covariance, le relevé est divisé en pixels HEALPix, en utilisant $\texttt{nside} = \num{16}$. Cette résolutivon produit des pixels d'une taille sur le ciel de $\num{3.7} \times \num{3.7} = \SI{13.4}{\square\deg}$, correspondant à $\num{250} \times \num{250} (\si{\perh\Mpc})^2$ à un redshift $z = \num{2.33}$. Ces sous-échantillons sont suffisamment grands pour pouvoir négliger les corrélations entre différents pixels HEALPix et ainsi estimer la matrice de covariance comme la variance d'un sous-échantillon à un autre. La matrice de covariance est donc calculée comme
\begin{equation}
  C_{AB} = \frac{1}{W_A W_B} \sum_s W_A^s W_B^s \left( \xi_A^s \xi_B^s - \xi_A \xi_B \right) \; ,
\end{equation}
où $s$ est un sous-échantillon, et $W_A^s$ les poids du bin $A$ de ce sous-échantillon.
Les éléments les plus importants de cette matrice sont les éléments diagonaux : la variance dans chaque bin. Les éléments non-diagonaux, les covariances entre deux bins distincts, sont faibles (\#prov donner comme Var\_A est modélisé ? Donner comment on modélise les éléments non diag ?). Leur estimation est bruitée. La matrice de covariance est donc lissée après avoir été estimée.
En ce qui concerne l'auto-corrélation, la matrice de covariance possède $\num{2500} \times \num{2500}$ bins. Pour la corrélation croisée, elle en possède $\num{5000} \times \num{5000}$.



\section{Modélisation des fonctions de corrélation}
Dans cette section, nous présentons les modèles utilisés pour ajuster les fonctions de corrélation \lya{}$\times$\lya{} et \lya{}$\times$QSO. Nous présentons d'abord les modèles utilisés dans l'analyse des données DR16, puis nous donnons les modèles utilisés pour analyser les mocks.

\subsection{Modélisation des données}
\label{subsec:model_donnees}
Pour l'analyse des données DR16, dont les résultats sont présentés dans le chapitre suivant, nous utilisons le modèle décrit dans~\textcite{prov}. L'analyse décrite dans cette étude est une analyse BAO : l'auto-corrélation et la corrélation croisée sont modélisées de façon à mesurer au mieux les paramètres BAO $\apar{}$ et $\aperp{}$.
Pour ce faire, le modèle est séparé en deux composantes. La première, $\xi_{smooth}$, correspond à la forme globale de la fonction de corrélation. 
La seconde, $\xi_{peak}$, correspond au pic BAO. C'est cette seconde composante qui dépend des paramètres BAO :
\begin{equation}
  \xi(\rpar{}, \rperp{}, \apar{}, \aperp{}) = \xi_{\mathrm{smooth}}(\rpar{}, \rperp{}) + \xi_{\mathrm{peak}}(\apar{} \rpar{}, \aperp{} \rperp{}) \; .
\end{equation}
% Cette séparation est opérée au niveau du spectre de puissance. Le modèle de la fonction de corrélation est ensuite obtenue à l'aide d'une transformation de Fourier.
\textbf{Cette distinction entre les composantes $smooth$ et $peak$ est faite lors du calcul du spectre de puissance modèle (voir paragraphe suivant). Le modèle de la fonction de corrélation est ensuite obtenue à l'aide d'une transformation de Fourier inverse de ce spectre de puissance.}
% Nous donnons dans les lignes qui suivent, comment nous construisons le spectre de puissance utilisé dans la modélisation de la fonction de corrélation des traceurs $i$ et $j$.
% Celui-ci s'exprime comme
% \subsubsection{Le spectre de puissance}
\paragraph{}
Le spectre de puissance utilisé dans la modélisation de la fonction de corrélation des traceurs $i$ et $j$ s'exprime comme
\begin{equation}
  \label{eq:pk_model1}
  P(\vec k) = b_i b_j (1+\beta_i \mu_k^2)(1+\beta_j \mu_k^2) P_{\mathrm{QL}}(\vec k) F_{\mathrm{NL}}(\vec k) G(\vec k) \; .
\end{equation}
% où $b_i$ est le biais du traceur $i$, $\beta_i$ le paramètre RSD du traceur $i$. $P_{QL}$ donne le spectre de puissance \emph{quasi-linéaire}. $F_{NL}$ prend en compte les non linéarités.
Les termes $b_i (1+\beta_i \mu_k^2)$ et $b_j (1+\beta_j \mu_k^2)$ sont les facteurs de Kaiser (équation~\ref{eq:kaiser3}) relatifs aux traceurs $i$ et $j$.
$P_{\mathrm{QL}}$ est le spectre de puissance \emph{quasi-linéaire}. Il est découpé en deux composantes $P_{smooth}$ et $P_{peak}$ comme
\begin{equation}
  P_{\mathrm{QL}}(\vec k, z) = P_{\mathrm{smooth}}(k, z) + \exp(- \frac{k_{\parallel}^2 \Sigma_{\parallel}^2 + k_{\perp}^2 \Sigma_{\perp}^2}{2}) P_{\mathrm{peak}}(k,z) \; .
\end{equation}
$P_{\mathrm{smooth}}$ est le spectre de puissance linéaire, sans les BAO. Il est construit à partir du spectre de puissance linéaire $P_{\mathrm{L}}$ donné par Camb, puis les BAO sont retirées en utilisant la technique \emph{side-band} décrite dans \textcite{Kirkby2013}.
Le spectre de puissance $P_{\mathrm{peak}}$ est alors obtenu comme la différence $P_{\mathrm{L}} - P_{\mathrm{smooth}}$ : il contient uniquement les oscillations dues aux BAO présentes dans le spectre de puissance linaire.
Le terme exponentiel devant $P_{\mathrm{peak}}$, paramétré par $\Sigma_{\parallel}$ et $\Sigma_{\perp}$, prend en compte l'élargissement non linéaire du pic BAO. Nous utilisons $\Sigma_{\parallel} = \SI{6.42}{\perh\Mpc}$ et $\Sigma_{\perp} = \SI{3.26}{\perh\Mpc}$  \autocite{eisenstein_robustness_2007}.
Le terme $F_{\mathrm{NL}}$ prend en compte les non linéarités aux petites échelles. Nous distinguons $F_{\mathrm{NL}}^{\mathrm{auto}}$ et $F_{\mathrm{NL}}^{\mathrm{cross}}$. Pour l'auto-corrélation, les effets non linéaires proviennent de l'élargissement thermique, des vitesses particulières et de la croissance des structures non linéaire.
Comme lors de la modélisation du $P^{\mathrm{1D}}$, nous utilisons le modèle décrit dans \textcite{Arinyo-i-Prats2015}. Nous avons donc $F_{\mathrm{NL}}^{\mathrm{auto}}(k, \mu) = D(k, \mu)$, où $D$ est défini dans l'équation~\ref{eq:p1d_prats}. Les paramètres utilisés sont une interpolation à $z = \num{2.334}$ de ceux donnés dans la section ``Planck'' de la table 7 de \textcite{Arinyo-i-Prats2015}.
Pour la corrélation croisée, l'effet dominant est dû aux vitesses non linéaires des quasars. Cet effet est modélisé par une lorrentzienne :
\begin{equation}
  F_{\mathrm{NL}}^{\mathrm{cross}}(\kpar{}) = \frac{1}{1 + (\kpar{} \sigma_v)^2} \; ,
\end{equation}
où l'inverse de la demi-largeur à mi-hauteur $\sigma_v$ est un paramètre libre. L'effet dû aux erreurs statistiques sur la mesure du redshift des quasars étant confondu avec l'effet des vitesses non linéaires des quasars, il est aussi pris en compte par le terme $  F_{\mathrm{NL}}^{\mathrm{cross}}$.
Enfin, le terme $G(\vec k)$ prend en compte l'effet du binning utilisé lors du calcul de la fonction de corrélation.
Il est défini comme le produit des transformés de Fourier de la fonction porte :
\begin{equation}
  G(\vec k) = \mathrm{sinc}\left(\frac{\kpar{}R_{\parallel}}{2}\right)\mathrm{sinc}\left(\frac{\kperp{}R_{\perp}}{2}\right) \; ,
\end{equation}
avec $R_{\parallel}$ et $R_{\perp}$ la largeur des bins, soit \SI{4}{\perh\Mpc}.

Afin de modéliser la corrélation croiśee \lya{}$\times$QSO, nous ajoutons le paramètre $\Delta_{\rpar{}, \textsc{QSO}}$, inclus comme
\begin{equation}
  \rpar{} = \rpar{}_{mesure} + \Delta_{\rpar{}, \textsc{QSO}} \; ,
\end{equation}
où $\rpar{}_{mesure}$ est la séparation mesurée des paires $(i,j)$, et $\rpar{}$ est la séparation utilisée dans le modèle de la corrélation croisée. L'ajout de ce paramètre permet de prendre en compte les erreurs systématiques sur la mesure du redshift des quasars, qui rendent assymétrique la fonction de corrélation \lya{}$\times$QSO.

% \subsubsection{Modélisation des HCD}
\paragraph{}
La présence des facteurs de Kaiser dans l'équation~\ref{eq:pk_model1} permet de mesurer le biais et le paramètre RSD de nos traceurs. En ce qui concerne l'auto-corrélation \lya{}$\times$\lya{}, la fonction de corrélation est proportionnelle à $b_{\mathrm{Ly}\alpha}^2(1+\beta_{\mathrm{Ly}\alpha} \mu^2)^2$. Cependant, la présence de HCD dans les données modifie le biais et le paramètre RSD du \lya{}. En effet, l'efficacité de l'algorithme de détection n'étant pas de \SI{100}{\percent}, il subsiste des DLA non identifiés dans les forêts. De plus, les HCD avec $\log n_{\textsc{HI}} < \num{20.3}$ ne sont pas identifiés. Ces deux effets participent à augmenter le biais mesuré du \lya{} significativement. Nous utilisons les paramètres effectifs
\begin{align}
  b_{\mathrm{Ly}\alpha}' &= b_{\mathrm{Ly}\alpha} + b_{\textsc{HCD}} F_{\textsc{HCD}}(\kpar{}) \; , \\
  b_{\mathrm{Ly}\alpha}' \beta_{\mathrm{Ly}\alpha}' &= b_{\mathrm{Ly}\alpha} \beta_{\mathrm{Ly}\alpha} + b_{\textsc{HCD}} \beta_{\textsc{HCD}} F_{\textsc{HCD}}(\kpar{})  \; ,
\end{align}
où $F_{\textsc{HCD}}$ est une fonction qui dépend de la distribution en $z$ et en $\log n_{\textsc{HI}}$ des HCD \autocite{Font-Ribera2012a}. Cette fonction est estimée sur des simulations hydrodynamiques \autocite{Rogers2017}. Elle est modélisée comme
\begin{equation}
  F_{\textsc{HCD}}(\kpar{}) = \exp(- L_{\textsc{HCD}} \kpar{}) \; ,
\end{equation}
où $L_{\textsc{HCD}}$ est la taille typique des HCD non masqués. La résolution du spectrographe d'eBOSS rend possible l'identification des DLAs dont la largeur est supérieure à \SI{2}{\nano\meter}, correspondant à une taille d'environ \SI{14}{\perh\Mpc} au redshift effectif de la mesure. Par ailleurs , $L_{\textsc{HCD}}$ est très dégénéré avec avec les autres paramètres du modèle, comme le biais des HCD ou les paramètres du \lya{}. Nous fixons donc sa valeur à \SI{10}{\perh\Mpc} dans l'ajustement du modèle.

Afin de pouvoir ajuster le même modèle sur tous les bins $(\rpar{}, \rperp)$, la dépendance en redshit de $\delta_F$ est prise en compte.
\textbf{En considérant que $\beta_{\mathrm{Ly}\alpha}$ est constant avec le redshift, nous avons $\delta_F(z) \propto G(z) b_{\mathrm{Ly}\alpha}(z)$, avec $G(z) \propto (1+z)^{-1}$.}
Concernant le biais du \lya{}, nous utilisons la même dépendance que celle choisie lors du calcul des poids (équation~\ref{eq:wieghts2}), c'est à dire $b_{\mathrm{Ly}\alpha} \propto (1+z)^{\gamma_{\mathrm{Ly}\alpha}}$, avec $\gamma_{\mathrm{Ly}\alpha} = 2.9$ \autocite{McDonald2004}.
En ce qui concerne $\beta_{\mathrm{Ly}\alpha}$, nous considérons lors de l'ajustement du modèle qu'il est indépendant du redshift. Comme montré dans l'analyse présentée dans le chapitre~\ref{prov}, $\beta_{\mathrm{Ly}\alpha}$ n'est pas indépendant du redshift. Cependant, le redshift moyen dans chaque bin $(\rpar{}, \rperp)$ varie peu. Lors de l'analyse de l'ensemble des données DR16, il varie dans la gamme $\num{2.31} < z < \num{2.39}$. Cette variation correspond à une variation de $\beta_{\mathrm{Ly}\alpha}$ de moins de \SI{5}{\percent}. De plus, elle est d'autant plus faible lorsque l'analyse est faite dans différents bins en redshift.

La fonction de corrélation croisée \lya{}$\times$QSO est sensible au produit $b_{\mathrm{Ly}\alpha}(1+\beta_{\mathrm{Ly}\alpha} \mu^2) b_{\textsc{QSO}}(1+\beta_{\textsc{QSO}} \mu^2)$.
\textbf{L'ajustement de cette seule fonction de corrélation ne permet donc pas de lever la dégénéréscence des paramètres \lya{} et des paramètres des QSO.}
Nous fixons donc $b_{\textsc{QSO}}$ et $\beta_{\textsc{QSO}}$. Concernant le biais des quasars, comme pour le \lya{} nous prenant en compte l'évolution avec le redshift. Il est paramétrisé comme
\begin{equation}
  b_{\textsc{QSO}}(z) = \num{3.77} \left( \frac{1 + z}{1+\num{2.334}} \right)^{\num{1.44}} \; . 
\end{equation}
% Cette paramétrisation est sensiblement équivalente à celle utilisée dans les mocks (équation~\ref{eq:b_qso}) dans la gamme $\num{2.31} < z < \num{2.39}$. $\beta_{\textsc{QSO}}$ est choisi constant et vaut $\beta_{\textsc{QSO}} = f / b_{\textsc{QSO}}(\num{2.334}) = \num{0.257}$.
\textbf{Cette paramétrisation est celle choisie dans \textcite{prov}. Nous gardons cette paramétrisation pour notre analyse des données, présentée dans le chapitre suivant. Cependant, lorsque nous analysons les mocks, nous utilisons la paramétrisation adoptée pour construire les mocks. Elle est donnée dans l'équation~\ref{eq:b_qso}.
$\beta_{\textsc{QSO}}$ est choisi constant et vaut $\beta_{\textsc{QSO}} = f / b_{\textsc{QSO}}(z_{eff})$, où $z_{eff}$ est le redshift effectif de la mesure. Le taux de croissance $f$ est aussi choisi constant avec le redshift, et vaut $f = \num{0.9704}$.}

\paragraph{}
Une fois les composantes multiplicatives incluses au modèle, nous pouvons transformer le $P(\vec k)$ modèle définit dans l'équation~\ref{eq:pk_model1} en fonction de corrélation $\xi(r, \mu)$.
% Cette transformation est faite en utilisant FFTLog \autocite{Hamilton1999} :
Pour ce faire,
la fonction de corrélation est décomposée en polynômes de Legendre $P_l$ jusqu'à $l_{max} = 6$. Pour chaque $l \in [0 \, ; 2 \, ; 4\, ; 6]$, une transformation de Fourier est appliquée au spectre de puissance (\#prov en fait c'est plutôt une intégrale avec une fonction de Bessel. Donner une ref du papier d'Hamilton ? C'est aussi dans le papier Kirkby, eq 2.7 - 2.9). Etant donné que la transformation $P(\vec k) \rightarrow \xi(r, \mu)$ est faite à chaque étape de la minimisation lors de l'ajustement du modèle, il est important que cette transformation se fasse très rapidement. Nous utilisons donc l'algorithme FFTLog \autocite{Hamilton1999}, qui apporte à la fois rapidité et précision.
Ainsi, en suivant cette procédure, nous obtenons les fonctions de corrélation $\xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha}$ en choisissant $i = j = \mathrm{Ly}\alpha$, et $\xi_{\mathrm{Ly}\alpha\times\textsc{QSO}}$ en choisissant $i = \mathrm{Ly}\alpha$ et $j = \textsc{QSO}$.
Afin de pouvoir ajuster les fonctions de corrélation calculées avec les données, nous devons prendre en compte dans nos modèles les corrélations parasites. Ces corrélations s'ajoutent au $\xi$ modèle calculé précédemment.
A ce stade, nous distingons le modèle utilisé pour l'auto-corrélation et la corrélation croisée.
Le modèle de l'auto-corrélation \lya{}$\times$\lya{} est défini comme
\begin{equation}
  \label{eq:cf_model1}
  % \xi = \xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha} + \sum_{m} \xi_{\mathrm{Ly}\alpha\times m} + \sum_{m, n} \xi_{m\times n} + \xi_{ciel}  \; ,
  \xi = \xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha}  + \sum_{m, n} \tilde \xi_{m\times n} + \xi_{ciel}  \; ,
\end{equation}
% où $\xi_{\mathrm{Ly}\alpha\times m}$ est la corrélation du \lya{} avec le métal $m$, $\xi_{m_1\times m_2}$ est la corrélation du métal $m_1$ avec le métal $m_2$, et $\xi_{ciel}$ est la corrélation induite par le masquage des lignes de ciel.
\textbf{où $\tilde \xi_{m \times n}$ est la corrélation de l'absorbeur $m$ avec l'absorbeur $n$, interprétés comme des absorptions \lya{}.
  Nous verrons plus tard comment relier la  corrélation $\tilde \xi_{m \times n}$ à la corrélation physique $\xi_{m \times n}$.
  Ces absorbeurs peuvent être du \lya{} ou des métaux ($m$ et $n$ ne peuvent pas être tous les deux du \lya{}). Le tableau~\ref{tab:metals_in_mocks} liste les métaux ajustés dans les données. $\xi_{ciel}$ est la corrélation induite par la soustraction du fond de ciel. Ces termes sont décrits dans les prochains paragraphes.}
Le modèle de la corrélation croisée \lya{}$\times$QSO est défini comme
\begin{equation}
  \label{eq:xcf_model1}
  \xi = \xi_{\mathrm{Ly}\alpha\times \textsc{QSO}} + \sum_{m} \tilde \xi_{m \times \textsc{QSO}} + \xi_{prox} \; ,
\end{equation}
où $\tilde \xi_{m \times \textsc{QSO}}$ est la corrélation du métal $m$ (interprété comme du \lya{}) avec les quasars, et $\xi_{prox}$ donne la corrélation induite par l'effet de proximité des quasars. Cest termes sont décrits dans les prochains paragraphes.

\paragraph{}
\textbf{Afin de modéliser la corrélation des métaux, nous utilisons le modèle défini précédemment, utilisé pour décrire les corrélations $\xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha}$ et $\xi_{\mathrm{Ly}\alpha\times\textsc{\textsc{QSO}}}$.
% Lors du calcul du spectre de puissance, nous négligeons l'effet des HCD, ce qui revient à utiliser $b_{\textsc{HCD}} = 0$.
Comme expliqué dans la section~\ref{subsec:contaminants}, toutes les absorptions sont supposées être des absorptions \lya{}. Les absorptions causées par les métaux sont donc reconstruites à un mauvais redshift. Ceci résulte dans un décalage de la fonction de corrélation le long de la ligne de visée.
% Pour la corrélation d'un métal $m$ avec le \lya{}, le décalage se fait le long de la ligne de visée : lorsque la séparation physique du métal et du \lya{} est nulle, la séparation reconstruite est $\rperp{} = 0$ et $\rpar{} \sim (1+z)D_H(z)(\lambda_m - \lambda_{\mathrm{Ly}\alpha} ) / \lambda_{\mathrm{Ly}\alpha}$.
% De manière générale, pour la corrélation d'un absorbeur $m$ avec un autre absorbeur $n$ ($m \neq n$), le décalage se fait le long de la ligne de visée. Lorsque la séparation physique de ces deux absorbeurs est nulle, la séparation, en supposant que ces deux absorptions sont causées par le \lya{}, est reconstruite à $\rperp{} = 0$ et $\rpar{} \sim (1+z)D_H(z)(\lambda_m - \lambda_{n} ) / \lambda_{\mathrm{Ly}\alpha}$, où $z$ est le redshift moyen des deux absorbeurs.
% La fonction de corrélation étant maximale pour $r = 0$, nous attentons donc un excès de corrélation pour ces séparations dans les fonctions de corrélation estimées à partir des données. Les métaux étant beaucoup moins présents que l'hydrogène dans le milieu intergalatique, les corrélations mettant en jeu deux métaux sont beaucoup moins importantes que les corrélations \lya{}$\times m$ ($m \neq$ \lya). L'effet principal vient donc des corrélations \lya{}$\times m$.
Considérons deux absorbeurs $m$ et $n$ ($m \neq n$). La fonction de corrélation $\xi_{m \times n}$ de ces deux absorbeurs étant maximale pour les séparations $r = 0$, nous attentons un excès de corrélation pour ces séparations.
Cependant, lorsque la séparation physique de ces deux absorbeurs est nulle, la séparation, en supposant que ces deux absorptions sont causées par le \lya{}, est reconstruite à $\rperp{} = 0$ et $\rpar{} \sim (1+z)D_H(z)(\lambda_m - \lambda_{n} ) / \lambda_{\mathrm{Ly}\alpha}$, où $z$ est le redshift moyen des deux absorbeurs. Ainsi, l'excès de corrélation observé n'est pas situé à $r = 0$, mais se trouve décalé le long de la ligne de visée.
Le tableau~\ref{tab:metals_in_mocks} donne les séparations associées aux corrélations entre le \lya{} et les métaux ajustés sur les données.
Les métaux étant beaucoup moins présents que l'hydrogène dans le milieu intergalactique, les corrélations mettant en jeu deux métaux sont beaucoup moins importantes que les corrélations mettant en jeu un métal et le \lya{}. % \lya{}$\times m$ ($m \neq$ \lya).
L'effet principal vient donc des corrélations $\xi_{\mathrm{Ly}\alpha \times m}$. % \lya{}$\times m$.
% Les pics associés à ces séparations sont visibles sur la figure~\ref{prov}.
En ce qui concerne les corrélations $\xi_{m \times m}$, le décalage est d'origine différente. Une séparation physique $r=0$ correspond bien à une reconstruction $\rpar{} = \rperp{} = 0$. Cependant, le redshift de la paire est mal estimé. Chaque séparation physique $(\rpar{}, \rperp{})$ est donc reconstruite à $(D_H(z_m) / D_H(z))\rpar{}$ et $(D_M(z_m)/D_M(z))\rperp{}$.
Pour la corrélation $\xi_{m \times \textsc{QSO}}$, le décalage est le même que dans le cas $\xi_{m\times n}$, en prenant $z_n = z_{\textsc{QSO}}$.
}
% Le décalage de la fonction de corrélation est pris en compte par la \emph{matrice des métaux} $M_{AB}^{m\times n}$. Cette matrice permet de relier la fonction de corrélation $\xi_{m\times n}$ de l'absorbeur $m$ avec l'absorbeur $n$ à la fonction de corrélation décalée, utilisée comme modèle dans l'équation~\ref{eq:cf_model1} et~\ref{eq:xcf_model1} :
% \begin{equation}
%   \xi^{modèle}_{m \times n}(A) = \sum_B M_{AB}^{m\times n} \xi_{m\times n}(B) \; ,
% \end{equation}
% Les bins $A$ correspondent aux séparations calculées en supposant une absorption \lya{}. Les bins $B$ correspondent aux séparations physiques, calculées en utilisant les redshifts $z_m$ et $z_n$ des absorbeurs.
% Le calcul de la matrice des métaux n'est pas détaillé ici
% Ainsi, pour chaque couple $m\times n$, la matrice $M_{AB}^{m\times n}$ est calculée.

Pour chaque couple $(m, n)$, le décalage de la fonction de corrélation $\xi_{m\times n}$ est pris en compte par la matrice des métaux $M_{AB}^{m\times n}$.
% Cette matrice inclut donc le calcule du décalage des séparations décrit précédemment.
Nous ne détaillons pas son calcul ici, mais il est donné dans la thèse \textcite{CITE:Victoria}. La matrice des métaux permet donc de relier la fonction de corrélation $\xi_{m\times n}$ à la fonction de corrélation $\tilde \xi_{m\times n}$, où les absorbeurs $m$ et $n$ sont interprétés comme des absorptions \lya{}, utilisée comme modèle dans l'équation~\ref{eq:cf_model1} et~\ref{eq:xcf_model1} :
\begin{equation}
  \tilde \xi_{m \times n}(A) = \sum_B M_{AB}^{m\times n} \xi_{m\times n}(B) \; ,
\end{equation}
Les bins $A$ correspondent aux séparations calculées en supposant une absorption \lya{}. Les bins $B$ correspondent aux séparations physiques, calculées en utilisant les redshifts $z_m$ et $z_n$ des absorbeurs.
Ainsi, pour chaque couple $(m, n)$, la matrice des métaux $M_{AB}^{m\times n}$ est calculée, puis la fonction de corrélation $\tilde \xi_{m \times n}$ est estimée et ajoutée à la fonction de corrélation $\xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha}$ (équation~\ref{eq:cf_model1}).
Dans le cas de la corrélation croisée, la matrice est calculée pour tous les couples $(m, \textsc{QSO})$, puis la fonction de corrélation $\tilde \xi_{m \times \textsc{QSO}}$ et estimée et ajoutée à la fonction de corrélation $\xi_{\mathrm{Ly}\alpha\times\textsc{QSO}}$ (équation~\ref{eq:xcf_model1}).

Le modèle utilisé pour construire les fonctions de corrélation $\xi_{m\times n}$ et $\xi_{m\times \textsc{QSO}}$ et le même que celui utilisé pour construire les fonctions de corrélation $\xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha}$ et $\xi_{\mathrm{Ly}\alpha\times\textsc{QSO}}$ (équation~\ref{eq:pk_model1}) mais avec les paramètres $b_i$, $b_j$, $\beta_i$ et $\beta_j$ qui sont ceux des métaux. Du fait que les métaux sont mesurables principalement le long de la ligne de visée, il est difficile d'ajuster à la fois le biais et le paramètre RSD de chaque métal. Nous ajustons donc uniquement le biais de chaque métal, le paramètre RSD étant fixé à $\beta_m = \num{0.5}$. De plus, nous négligeons l'impact des HCD sur les fonctions de corrélations $\xi_{m\times n}$ et $\xi_{m\times \textsc{QSO}}$.

\begin{table}[]
  \centering
  \caption{Liste des métaux inclus dans le modèle ajusté aux données. La 3\up{e} colonne donne la séparation reconstruire pour une séparation réelle $r=0$. CIV(eff) indique la raie effective du carbon IV : la résolution du spectrographe d'eBOSS étant trop faible pour distinguer le doublet du CIV, nous ajustons la combinaison des deux raies. La séparation liée au CIV est bien supérieure à \SI{200}{\perh\Mpc}, la corrélation \lya{}$\times$CIV n'a donc pas d'effet sur nos mesures. Nous modélisons cependant l'effet lié à l'auto-corrélation CIV$\times$CIV.}
  \label{tab:metals_in_mocks}
  \begin{tabular}{lll}
    \toprule
    Raie  & $\lambda_{m} [\si{\angstrom}]$ & $\rpar{}$ [\si{\perh\Mpc}] \\
    \midrule
    % $\mathrm{Lyman-}\beta$ & \lyb{}  & \num{1025.72} \\
    % $\mathrm{Lyman-}\alpha$ & \lya{} & \num{1215.67} \\
    SiII(1190) &  \num{1190.4158} & $- 64$ \\
    SiII(1193) &  \num{1193.2897} & $- 56$  \\
    SiIII(1207) &  \num{1206.500}  & $- 21$  \\
    SiII(1260) &  \num{1260.4221} & $+ 111$ \\
    CIV(eff) & \num{1549.06} & $> 200$ \\
    % Silicium IV & SiIV(1394) & \num{1394.76018} \\
    % Silicium IV & SiIV(1403) & \num{1402.77291} \\
    % Carbon IV & CIV(1548) & \num{1548.2049} \\
    % Carbon IV & CIV(1551) & \num{1550.77845} \\
    % Magnesium II & MgII(2796) & \num{2796.3511} \\
    % Magnesium II & MgII(2804) & \num{2803.5324}\\
    \bottomrule
  \end{tabular}
\end{table}


\paragraph{}
Le terme additionnel suivant est le terme $\xi_{ciel}$. Ce terme prend en compte les corrélations induites par la soustraction du fond de ciel. Lors de la réduction des données, décrite dans la section~\ref{sec:reduction_donnees}, le spectre du fond de ciel est soustrait à tous les spectres d'une même demi-plaque. Ceci induit alors des corrélations entre tous ces spectres pour $\rpar{} = 0$. A cause de la distorsion induite par l'ajustement du continuum, cette effet ne se limite pas à $\rpar{} = 0$.
L'effet est modélisé par une fonction gausienne de $\rperp{}$ :
\begin{equation}
  \xi_{ciel}(\rpar{}, \rperp{}) =
  \left\{
    \begin{array}{ll}
      \frac{A_{sky}}{\sqrt{2 \pi \sigma_{sky}^{2}}} \exp(- \frac{\rperp{}^2}{2 \sigma_{sky}^2}) & , \hspace{0.2cm} \mathrm{si} \hspace{0.2cm} \rpar{} = 0 \\
      0 & ,  \hspace{0.2cm} \mathrm{si} \hspace{0.2cm}  \rpar{} \neq 0
    \end{array}
\right.  \; .
\end{equation}
Les paramètres $A_{sky}$ et $\sigma_{sky}$ sont laissés libres lors de l'ajustement des données. Ils donnent l'amplitude et la largeur de la gausienne.
Le terme $\xi_{ciel}$ n'est présent que dans l'ajustement de l'auto-corrélation, car ces corrélations parasites ne sont induites que lorsqu'on corrèle des pixels d'absorption issus de deux spectres présents sur la même demi-plaque. Cet effet n'a donc pas lieu d'être pour la corrélation croisée.

\paragraph{}
Enfin, le dernier terme additionnel est le terme $\xi_{prox}$. Ce terme n'est présent que dans la fonction de corrélation croisée $\xi_{\mathrm{Ly}\alpha\times\textsc{QSO}}$. Il prend en compte l'effet du rayonnement produit par les quasars sur l'hydrogène environnant. En effet, à cause de leur grande luminosité, et en particulier dans la direction de leur jet, les quasars ionisent le gaz qui les entoure. Ceci réduit donc la fraction d'hydrogène neutre et donc la profondeur optique au voisinage de chaque quasar, ce qui induit des corrélations supplémentaire entre le champ d'absorption \lya{} et la position des quasars. Cet effet est modélisé comme \autocite{Font-Ribera2013} :
\begin{equation}
  \xi_{prox} = \xi_{0,prox} \left(\frac{\SI{1}{\perh\Mpc}}{r} \right)^2 \exp( - \frac{r}{\lambda_{UV}}) \; ,
\end{equation}
où $\xi_{0,prox}$ donne l'amplitude de l'effet. L'émission est supposée isotrope, et le paramètre $\lambda_{UV}$ est fixé à \SI{300}{\perh\Mpc}.


\paragraph{}
Une fois tous ces termes inclus, nous obtenons un mondèle qui décrit la fonction de corrélation de la matière dans l'espace des redshifts, à un biais près, et qui prend en compte les différents effets astrophysiques ou instrumentaux qui affectent les données.
Afin de pourvoir correctement comparer notre modèle aux fonctions de corrélation calculées avec les données, nous devons prendre en compte la distorsion due à l'ajustement du continuum. Ceci est fait, comme décrit dans la section~\ref{sec:calcul_dmat}, grâce à la matrice de distorsion. Le modèle distordu est alors donné par
\begin{equation}
  \label{eq:model_dist}
  \xi_{\mathrm{distorsion}}(A) = \sum_{B}D_{AB}\xi(B) \; ,
\end{equation}
où $D_{AB}$ est la matrice de distorsion, et $\xi$ est le modèle construit précédemment.
Ainsi, le modèle qui est ajusté aux données est $\xi_{\mathrm{distorsion}}(A)$.

(\#prov je ne parle pas du broadband?)




\subsection{Modélisation des mocks}
Afin d'analyser les fonctions de corrélation \lya{}$\times$\lya{} et \lya{}$\times$QSO des mocks, nous utilisons les modèles ajustés sur les données et décrits dans la section précédente. Cependant, un certain nombre d'effets modélisés dans les données ne sont pas présents dans les mocks. Nous modifions donc légèrement les modèles décrits précédemment.

Premièrement, nous n'incluons pas l'élargissement non linéaire du pic BAO dans les mocks. Cet effet n'est pas non plus inclus par \texttt{quickquasars}. Lors du fit des mocks, nous forçons donc $\Sigma_{\perp} = \Sigma_{\parallel} = 0$.
Les non-linéarités prises en compte par le terme $F_{NL}$ ne sont pas non plus présentes dans nos mocks. Ainsi, le spectre de puissance défini dans l'équation~\ref{eq:pk_model1} et utilisé comme modèle pour l'auto-corrélation ne contient pas le terme $F_{NL}^{\mathrm{auto}}$. Cependant, le code \texttt{quickquasars} ajoute une vitesse particulière à chaque quasar. Ceci a pour effet d'ajouter une erreur statistique sur la mesure du redshfit des quasars. Nous gardons donc le terme $F_{NL}^{\mathrm{cross}}$ lorsque nous ajustons la corrélation \lya{}$\times$QSO issues des mocks avec \texttt{quickquasars}. Ce terme n'est pas présent dans le modèle utilisé pour ajuster les raw mocks.

L'effet instrumental causé par la soustraction du fond de ciel sur l'auto-corrélation n'est pas modélisé par \texttt{quickquasars}. Nous n'ajoutons donc pas le terme $\xi_{ciel}$ à $\xi_{\mathrm{Ly}\alpha\times\mathrm{Ly}\alpha}$ dans l'ajustement de l'auto-corrélation.
L'effet de proximité des quasars sur le champ \lya{} environnant n'est ajouté ni dans les mocks, ni dans \texttt{quickquasars}. Le terme
$\xi_{prox}$ n'est donc pas inclu dans le modèle de la corrélation croisée.

En ce qui concerne l'ajustement des HCD et des métaux, cela dépend de la version des mocks analysée.
Pour les raw mocks comme pour les mocks eboss-0.0, ni les HCD ni les métaux ne sont présents. Nous n'incluons donc pas leur modélisation dans ces versions des mocks.
Les versions eboss-0.2 incluent les HCD, nous modélisons donc leur présence comme décrit dans la section précédente.
Les versions eboss-0.3 incluent à la fois les HCD et les métaux. L'ajustement des fonctions de corrélation issues de ces mocks contient donc les paramètres $b_{\textsc{HCD}}$ et $\beta_{\textsc{HCD}}$. De plus, nous calculons la matrice des métaux pour ces versions et ajoutons au modèle les termes $\xi_{m\times n}$ et $\xi_{m\times \textsc{QSO}}$.

Pour l'ajustement des corrélations croisées \lya{}$\times$QSO, nous gardons le paramètre $\Delta_{\rpar{}, \textsc{QSO}}$. Même si \texttt{quickquasars} ajoute une erreur statistique sur les redshifts des quasars, cette erreur est nulle en moyenne. Nous nous attendons donc à obtenir $\Delta_{\rpar{}, \textsc{QSO}} = 0$ dans l'ajustement des différentes versions des mocks. Ainsi, $\Delta_{\rpar{}, \textsc{QSO}}$ sert de test de la construction des mocks, pour vérifier par exemple que les lignes de visées sont placées correctement à partir de chaque quasar.

\paragraph{}
Comme pour les données, les fonctions de corrélations sont évaluées sur une grille de séparation d'intervale \SI{4}{\perh\Mpc}. Nous gardons donc le terme $G(\vec k)$ dans la modélisation des mocks.
Dans le cas des mocks, nous incluons un terme supplémentaire, qui prend en compte le lissage gaussien appliqué au champ $\delta_l$ interpolé. Ce terme est donné par
\begin{equation}
  W(\kpar{}, \kperp{}) = \exp(- \frac{(\kpar{} \sigma^{smooth}_{\parallel})^2 + (\kperp{} \sigma^{smooth}_{\perp})^2}{2}) \; .
\end{equation}
Le spectre de puissance modèle $P(\kpar{}, \kperp{})$ est donc multiplié par $W^2(\kpar{}, \kperp{})$.

Enfin, similairement à la modélisation des données, le modèle ajusté sur les mocks est multiplié par la matrice de distorsion $D_{AB}$ (équation~\ref{eq:model_dist}). Cependant, dans le cas des raw mocks, les fonctions corrélation ne sont pas affectées par la distorsion due à l'ajustement du continuum puisque nous avons accès directement au champ $\delta_F$. Dans ce cas, la matrice de distorsion $D_{AB}$ vaut la matrice identité.


\section{Analyse des mocks}
Comme expliqué dans la section~\ref{sec:quickquasars}, nous produisons différentes versions des mocks : les raw mocks, pour lesquels le champ $\delta_F$ est obtenu directement à partir des vrais transmissions, et les mocks après l'utilisation de \texttt{quickquasars} : eboss-0.0, eboss-0.2 et eboss-0.3.
Nous présentons dans cette section l'analyse des différentes versions des mocks.


\subsection{Analyse des raw mocks}
% Nous présentons premièrement l'analyse des raw mocks.
% Pour les cents réalisations produites, nous avons calculé l'auto corrélation et la corrélation croisée des raw mocks.
Pour chacune des cent réalisations produites, nous avons calculé les fonctions de corrélation \lya{}$\times$\lya{} et \lya{}$\times$QSO des raw mocks.
L'analyse de ces fonctions de corrélation permet d'identifier plus facilement les problèmes qui peuvent exister au niveau de la construction des mocks, car les effets astrophysiques et instrumentaux introduits par \texttt{quickquasars} ne sont pas présents. Nous commençons donc par valider la construction des mocks, via l'étude des raw mocks, puis nous présenterons l'analyse des versions des mocks avec quickquasars.

\subsubsection{L'auto-corrélation \lya{}$\times$\lya{}}
La figure~\ref{prov} (\#prov faire la figure) donne l'addition des cent fonctions de corrélation \lya{}$\times$\lya{} des raw mocks.
La figure montre la fonction de corrélation dans quatre bins en $\mu$ différents. Le bin $\num{0.95} < \mu < 1$ correspond aux paires avec une séparation le long de la ligne de visée. Le bin $\num{0} < \mu < \num{0.5}$ correspond aux paires perpendiculaires à la ligne de visée.
Sur cette figure, nous pouvons voir la fonction de corrélation des raw mocks (couleur), ainsi que l'ajustement du modèle produit par \texttt{picca} (couleur) et la prédiction des mocks (couleur).
\#prov  conclusions


\paragraph{}
\#prov Est-ce qu'il faut pas que le paragraphe qui suit soit pour l'ajustement combiné de la CF et XCF ?
Sachant que le biais et beta lya sont déterminés sur le fit en 4 bins de la CF des données, c'est peut-être mieux de faire uniquement sur la CF. Je peux ensuite montrer l'évolution du biais des QSO en fitant la XCF, avec les parametres lya fixés à ce que je trouve dans le fit de la CF ? Ou alors je fais le fit combiné, mais dans ce cas là si une partie du lya passe dans les QSO (ou inversement), j'aurais pas le bon biais QSO.

Nous présentons maintenant le résultat de l'analyse en quatre bins en redshift des raw mocks. Pour chacune des réalisations, les fonctions de corrélation ont été calculées dans quatre bins en redshift distincts. Une fois toutes ces fonctions de corrélation produites, nous avons calculé l'addition des cent fonctions de corrélation dans chacun des quatres bins en redshift. Puis, nous avons ajustés ces quatre fonctions de corrélation séparément avec \texttt{picca}.
Le tableau~\ref{prov} (\#prov faire le tableau) donne le résultat de l'ajustement dans chaque bin en redshift.
La figure~\ref{prov} (\#prov faire la figure) montre le biais et le paramètre RSD du \lya{} obtenus dans l'ajustement des cent raw mocks (couleur), ainsi que ceux obtenus dans l'ajustement des données (couleur).
L'analyse en quatre bins en redshift des données est décrite dans la section~\ref{prov}. Pour chaque bin en redshift, nous générons la prédiction au redshift effectif du bin. Les biais et paramètres RSD mesurés sur la prédiction sont montrés en (couleur).
Enfin, pour chacun des jeux des données montrés sur la figure~\ref{prov}, nous ajustons une fonction puissance $y(z) = a (1+z)^{\gamma}$. Le paramètre $\gamma$ nous donne la dépendance en redshift de $b_{\mathrm{Ly}\alpha}$ et $\beta_{\mathrm{Ly}\alpha}$ dans chaque cas. Les différents $\gamma$ obtenus sont résumés dans le tableau~\ref{prov} (\#prov faire le tableau).
\#prov conclusions

\#prov montrer <F>(z) ou au moment du tuning

\subsubsection{La corrélation croisée \lya{}$\times$QSO}

Nous présentons maintenant la même analyse que précédemment, mais en considérant cette fois ci les cent fonctions de corrélation \lya{}$\times$QSO calculées sur les raw mocks. La figure~\ref{prov} (\#prov faire la figure) montre l'addition de ces cent fonctions de corrélation croisées. La fonction de corrélation est montrée dans quatre bins en redshift. La ligne continue (couleur) donne l'ajustement du modèle produit par \texttt{picca}. La prédiction est montrée en (couleur).
\#prov conclusion


\paragraph{}
Comme précédemment, nous présentons l'analyse en quatre bins en redshift.
Le tableau~\ref{prov} (\#prov faire le tableau) donne le résultat de l'ajustement dans chaque bin en redshift. Pour cet ajustement, nous avons fixé les paramètres \lya{} à ce que nous avons obtenu dans la section précédente. Ceci nous permet d'obtenir une mesure des paramètres des quasars moins corrélée avec la mesure des paramètres \lya{}. (\#prov donner les correlations de bLya et betaLya avec les autres paramètres ?).
La figure~\ref{prov} (\#prov faire la figure) montre le biais et le paramètres RSD des quasars obtenus avec un tel ajustement.
La ligne en pointillés donne le biais $b_{\textsc{QSO}}$ utilisé pour construire le champ des quasars (voir section~\ref{subsec:qso}).
\#prov conclusion



\subsubsection{Le spectre de puissance à une dimension}

\#prov montrer le P1D des raw mocks ? Plutot celui obtenu avec quickquasars ?
oui


\subsubsection{L'auto-corrélation QSO$\times$QSO}
Nous présentons ici la mesure de l'auto-corrélation QSO$\times$QSO, estimée grâce à l'équation~\ref{prov}.
Comme pour les corrélations \lya{}$\times$\lya{} et \lya{}$\times$QSO, la fonction de corrélation QSO$\times$QSO est estimée puis ajustée dans les quatre bins en redshift $[\num{0}\,;\num{2.35}]$, $[\num{2.35}\,;\num{2.65}]$, $[\num{2.65}\,;\num{3.05}]$ et $[\num{3.05}\,;\num{10}]$.
Nous estimons la corrélation de dix réalisations des raw mocks, puis nous calculons et ajustons l'addition de ces dix fonctions de corrélation. La figure~\ref{fig:auto_qso_4bins} montre l'addition des dix fonctions de corrélation et leur modèle ajusté dans chaque bin en redshift. Pour chacun des bins, la fonction de corrélation est montrée dans trois bins en $\mu$.
\begin{figure}
  \centering
  \includegraphics[scale=0.5]{auto_qso_4bins}
  \caption{bla}
  \label{fig:auto_qso_4bins}
\end{figure}
Le modèle utilisé pour ajuster la corrélation QSO$\times$QSO est le même que celui utilisé pour ajuster la corrélation \lya{}$\times$\lya{}, à la différence que nous n'incluons pas le terme représentant le lissage gaussien. % Le biais et le paramètre RSD ajustés donnent les paramètres des quasars. Ils sont représentés sur la figure~\ref{prov}.
Dans le cas de la corrélation QSO$\times$QSO, contrairement au cas du \lya{}, nous avons la relation $b_{\textsc{QSO}} \beta_{\textsc{QSO}} = f$, où $f$ est le taux de croissance des structures. Lors de l'ajustement, nous fixons donc le paramètre $b_{\eta, \textsc{QSO}} = 1$, et nous ajustons les paramètres $\beta_{\textsc{QSO}}$ et $f$.

Sur la figure~\ref{fig:auto_qso_4bins}, nous pouvons remarquer que les mocks sont en très bon accord avec le modèle ajusté par picca. Une légère différence est visible à petit $r$. Cette différence dépend de $\mu$ : l'amplitude est trop faible le long de la ligne de visée, et trop importante perpendiculairement à cette dernière. Ceci semble provenir des RSD aux petites échelles.
La figure~\ref{fig:bias_auto_qso} montre $b_{\textsc{QSO}}$ et $\beta_{\textsc{QSO}}$ obtenus avec l'ajustement de l'addition des dix fonctions de corrélation.
La ligne bleue donne la paramétrisation utilisée dans les mocks (équation~\ref{eq:b_qso}).
Pour les petits redshifts, $b_{\textsc{QSO}}$ est trop faible et $\beta_{\textsc{QSO}}$ est trop grand grand.
Cependant, les valeurs ajustées de $\beta_{\textsc{QSO}}$ et $f$ sont corrélées à plus de \SI{99}{\percent}. Ceci vient du fait que, pour les quasars, le paramètre RSD est faible. Il y a donc peu de différences entre la corrélation le long de la ligne de visée et perpendiculairement à cette dernière.
Lorsque nous fixons $f$ à ce qui est donné par la cosmologie utilisée (équation~\ref{eq:par_cosmo}), nous obtenons un biais et un paramètre RSD en très bon accord avec la paramétrisation utilisée. La figure~\ref{fig:bias_auto_qso_fixed_f} montre $b_{\textsc{QSO}}$ et $\beta_{\textsc{QSO}}$ obtenus avec un tel ajustement.
\begin{figure}
  \centering
  \includegraphics[scale=0.4]{bias_auto_qso}
  \caption{bla}
  \label{fig:bias_auto_qso}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[scale=0.4]{bias_auto_qso_fixed_f}
  \caption{bla}
  \label{fig:bias_auto_qso_fixed_f}
\end{figure}

% La mesure de $b_{\textsc{QSO}}$ et $\beta_{\textsc{QSO}}$ est montrée sur la figure~\ref{prov}.

\subsubsection{La corrélation croisée \lya$\times$DLA}

\#prov montrer cette corrélation ? Dans le cas des raw mocks ?
Le faire avec quickquasars, et superposer ce qu'on obtient avec les données ?
a voir peut etre plutot l'auto DLAxDLA, pour verifier le biais qu'on obtient



\subsection{Analyse des mocks eboss-0.0}
Maintenant que nous avons vérifié que les raw mocks possédaient les bonnes fonctions de corrélation, nous pouvons analyser les mocks après avoir appliqué \texttt{quickquasars}.
Nous commençons par présenter l'analyse des mocks eboss-0.0.
% Afin de calculer le champ $\delta_F$, nous avons recours à l'ajustement du continuum. Les modèles incluent donc la matrice de distorsion. Contrairement aux raw mocks, les mocks issus de \texttt{quickquasars} contiennent du bruit instrumental. Les fonctions de corrélation sont donc plus bruitées.
Contrairement aux raw mocks, nous avons recours à l'ajustement du continuum pour calculer le champ $\delta_F$. Ainsi, les fonctions de corrélation possèdent les distorsions liées à cette ajustement, et les modèles sont multipliés par les matrices de distorsions (équation~\ref{eq:model_dist}). De plus, les mocks issus de \texttt{quickquasars} contiennent du bruit instrumental. Les fonctions de corrélation sont donc plus bruitées que celles calculées sur les raw mocks.


\subsubsection{L'auto-corrélation \lya{}$\times$\lya{}}
\#prov figure du stack des CF dans les 4 bins en mu + modèle picca lya only ou prédiction * dmat \\
+ tableau qui donne le résultat du fit ? \\
+ figure qui montre l'évolution avec z du biais et beta ? Le plot de la section précédente + le b et beta obtenu pour eboss-0.0 ; ou alors je fais un seul plot avec toutes les versions : raw mocks, eboss-0.0, eboss-0.2, eboss-0.3, prediction, data


\subsubsection{L'auto-corrélation \lya{}$\times$QSO}
\#prov figure du stack des XCF dans les 4 bins en mu. \\
+ tableau qui donne le résultat du fit ? fit avec les parametres lya fixés ?\\ 
+ figure qui montre l'évolution avec z du biais et beta ? Comme pour le lya, je met un plot à chaque fois, ou alors un seul plot qui résume tout ?


\subsubsection{Le spectre de puissance à une dimension}
\#prov montrer le P1D ?
bof, ca teste surtout l'analyse de michael


\subsection{Analyse des mocks eboss-0.2}
Nous analysons à présent les mocks eboss-0.2. Ces mocks sont obtenus comme les mocks eboss-0.0, analysés précédemment, à la différence que le code \texttt{quickquasars} inclue les HCD dans les spectres synthétiques. Nous modélisons donc les HCD lors de l'ajustement des fonctions de corrélation.
\#prov je me limite ici aux CF obtenues en masquant les DLA à partir du true catalogue ? Et je parlerai des differences dans le chapitre suivant ? 

\subsubsection{L'auto-corrélation \lya{}$\times$\lya{}}
\#prov figure du stack des CF dans les 4 bins en mu + modele picca lya + hcd \\
+ tableau qui donne le résultat du fit ? \\
+ figure qui montre l'évolution avec z du biais et beta ?


\subsubsection{L'auto-corrélation \lya{}$\times$QSO}
\#prov figure du stack des XCF dans les 4 bins en mu. \\
+ tableau qui donne le résultat du fit ? fit avec les parametres lya fixés ?\\ 
+ figure qui montre l'évolution avec z du biais et beta ? 


\subsubsection{Le spectre de puissance à une dimension}
\#prov montrer le P1D ? bof, pareil



\subsection{Analyse des mocks eboss-0.3}
Cette section présente l'analyse des mocks eboss-0.3. Ces mocks sont obtenus comme les mocks eboss-0.2, analysés précédemment, à la différence que le code \texttt{quickquasars} ajoute les métaux dans les spectres synthétiques. Nous incluons donc les termes $\xi_{m\times n}$ et $\xi_{m\times \textsc{QSO}}$ dans la modélisation des fonctions de corrélation.


\subsubsection{L'auto-corrélation \lya{}$\times$\lya{}}
\#prov figure du stack des CF dans les 4 bins en mu + modele picca lya + hcd + met \\
+ tableau qui donne le résultat du fit ? \\
+ figure qui montre l'évolution avec z du biais et beta ?


\subsubsection{L'auto-corrélation \lya{}$\times$QSO}
\#prov figure du stack des XCF dans les 4 bins en mu. \\
+ tableau qui donne le résultat du fit ? fit avec les parametres lya fixés ?\\ 
+ figure qui montre l'évolution avec z du biais et beta ? 


\subsubsection{Le spectre de puissance à une dimension}
\#prov montrer le P1D ?
Pourquoi pas, pour montrer les metaux ? Ou alors le xi1d

\subsubsection{La fonction de corrélation à une dimension}
Comparer les mocks et les données


% \bibliography{../source/library}
\printbibliography
\end{document}
